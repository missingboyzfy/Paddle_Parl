# Paddle_Parl百度强化学习心得

通过追百度举的强化学习培训，在科科老师的讲解下，对百度的Parl框架有深入的了解。对于一个初学深度学习的人来说，这次的课程浅显易懂。

这次培训对于强化学习的基本求解方法做了详细的介绍：

1）基于表格型的求解的Q-learning、Sarsa

2）基于神经网络的求解的DQN

3）基于策略梯度的求解的Policy Gradient

4）连续动作空间上求解的DDPG

自己也购买了基本书籍，但书上往往是对模型的数学推导进行解释，并没有过多的介绍应用场景。通过这次培训，从实践的角度出发，能真正的将理论与实践相结合，通过真正的动手实践，让我对强化学习有一个感性的认识，知道如何将强化学习应用于实际项目。

对于不同模型如何进行调参

学习率：一般调参经验是10倍10倍的调，然后看reward收敛哪个更快一些

gamma：gamma值越高，表示我们希望agent更加关注未来，这比更加关注眼前更难，因此训练更加缓慢和困难。有的环境需要看『远』一点才能训练的好，有些环境不需要看那么『远』也能达到一个不错的效果。

训练轮次：如果参数调的对，训练轮次越多，分数越高，直到收敛。最开始的时候我们可以让训练轮次多一点，看看分数到第几轮之后就不太上涨了

在学习的过程中，也认识了很多小伙伴，大家一起交流学习经验，如何调参，如何修改模型。虽然课程短暂，但真正的学到知识。
